{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from zipfile import ZipFile \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "\n",
    "# importing libraries for Deep Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import PIL\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir_walls = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/train_300/walls'\n",
    "train_dir_underpass = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/train_300/underpass'\n",
    "test_dir_walls = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/test_300/walls'\n",
    "test_dir_underpass = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/test_300/underpass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/train_300\"\n",
    "test_dir=\"C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/test_300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of walls training images -  325\n",
      "number of underpasss training images -  339\n",
      "number of walls testing images -  76\n",
      "number of underpass testing images -  72\n"
     ]
    }
   ],
   "source": [
    "print('number of walls training images - ',len(os.listdir(train_dir_walls)))\n",
    "print('number of underpasss training images - ',len(os.listdir(train_dir_underpass)))\n",
    "print('number of walls testing images - ',len(os.listdir(test_dir_walls)))\n",
    "print('number of underpass testing images - ',len(os.listdir(test_dir_underpass)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(rescale = 1.0/255.0,zoom_range= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 664 images belonging to 2 classes.\n",
      "Found 148 images belonging to 2 classes.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "training_data = data_generator.flow_from_directory(directory = train_dir,target_size = (100, 100),\n",
    "                                                   batch_size = batch_size,class_mode = 'binary')\n",
    "\n",
    "testing_data = data_generator.flow_from_directory(directory = test_dir,target_size = (100, 100),\n",
    "                                                  batch_size = batch_size,class_mode = 'binary')\n",
    "\n",
    "print(training_data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GridSearchCv\n",
    "\n",
    "def make_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (5, 5), activation = 'relu', input_shape = training_data.image_shape))\n",
    "    model.add(MaxPooling2D(pool_size = (4, 4)))\n",
    "    model.add(Dropout(rate = 0.4))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #model.add(Dropout(rate = 0.2))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #model.add(Dropout(rate = 0.2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    #model.add(Dropout(rate = 0.1))\n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(Dropout(rate = 0.4))\n",
    "\n",
    "\n",
    "    model.add(Dense(units = len(set(training_data.classes)), activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = KerasClassifier(build_fn=make_model)\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = np.array([50, 100, 150])\n",
    "batches = np.array([5, 10, 20])\n",
    "param_grid = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu', input_shape = training_data.image_shape))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.4))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "\n",
    "#model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu'))\n",
    "#model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "#model.add(Dropout(rate = 0.1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units = 32, activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.5))\n",
    "model.add(Dense(units = 16,activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(units = len(set(training_data.classes)), activation = 'softmax'))\n",
    "#model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier = KerasClassifier(build_fn=make_model)\n",
    "\n",
    "optimizer = ['adam','rmsprop']\n",
    "epochs = [15,25,30]\n",
    "steps_per_epoch = [20,30]\n",
    "validation_steps = [20,30]\n",
    "\n",
    "param_grid = dict(epochs=epochs,steps_per_epoch = steps_per_epoch,validation_steps = validation_steps,\n",
    "                 optimizer = optimizer)\n",
    "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, cv=3, scoring = ['accuracy'],refit = False)\n",
    "grid_result = grid.fit(training_data, testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = model.fit_generator(training_data,\n",
    "                        steps_per_epoch = 40,\n",
    "                        epochs = 20,\n",
    "                        validation_data = testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to load and fit\n",
    "\n",
    "import tensorflow as tf \n",
    "from keras.callbacks import History \n",
    "\n",
    "model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "model.load_weights('model_num.h5')\n",
    "\n",
    "\n",
    "fitted_model = model.fit_generator(training_data,\n",
    "                        steps_per_epoch = 22,\n",
    "                        epochs = 5,\n",
    "                        validation_data = testing_data,\n",
    "                        validation_steps = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy and validation accuracy\n",
    "accuracy = fitted_model.history['acc']\n",
    "plt.plot(range(len(accuracy)), accuracy, 'bo', label = 'accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "imgCat = cv2.imread('C:/Users/asa_f/SJSU/257/project/300x300/more test/underpass1.jpg')\n",
    "plt.imshow(imgCat)\n",
    "\n",
    "imgCat = cv2.resize(imgCat, (300,300))\n",
    "imgCat = imgCat.reshape(1,300,300,3)\n",
    "\n",
    "pred = model.predict(imgCat)\n",
    "print(\"Probability that it is a Cat = \", \"%.2f\" % (1-pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "def testing_image(image_directory):\n",
    "\n",
    "    test_image = image.load_img(image_directory, target_size = (100, 100))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = loaded_model.predict(x = test_image)\n",
    "    print(result)\n",
    "    if result[0][0]  >= 0.5:\n",
    "        prediction = 'wall'\n",
    "    else:\n",
    "        prediction = 'underpass'\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "direc = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/more test'\n",
    "\n",
    "\n",
    "for i in os.listdir(direc):\n",
    "\n",
    "    image_file = Image.open(direc + '/' + i) # open colour image\n",
    "    image_file = image_file.convert('1') # convert image to black and white\n",
    "    image_file.save('C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/' + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n",
      "Prediction of the image named underpass1.jpg is underpass.\n",
      "[[0. 1.]]\n",
      "Prediction of the image named underpass2.jpg is underpass.\n",
      "[[0. 1.]]\n",
      "Prediction of the image named underpass3.jpg is underpass.\n",
      "[[0. 1.]]\n",
      "Prediction of the image named underpass4.jpg is underpass.\n",
      "[[0. 1.]]\n",
      "Prediction of the image named underpass5.jpg is underpass.\n",
      "[[1. 0.]]\n",
      "Prediction of the image named wall1.png is wall.\n",
      "[[1. 0.]]\n",
      "Prediction of the image named wall2.png is wall.\n",
      "[[1. 0.]]\n",
      "Prediction of the image named wall3.png is wall.\n",
      "[[1. 0.]]\n",
      "Prediction of the image named wall4.png is wall.\n"
     ]
    }
   ],
   "source": [
    "for im in os.listdir(direc):\n",
    "    \n",
    "    #print('C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/' + im)\n",
    "\n",
    "    preds = testing_image('C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/' + im)\n",
    "    print(\"Prediction of the image named {} is {}.\".format(im,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_75.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "#  the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "\n",
    "\n",
    "with open(\"cnn_75_num.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"cnn_75_num.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import tensorflow as tf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('cnn_75.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16928)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                541728    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 552,434\n",
      "Trainable params: 552,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights('cnn_75_num.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40/40 [==============================] - 6s 160ms/step - loss: 0.3175 - acc: 0.8699 - val_loss: 0.1754 - val_acc: 0.9392\n",
      "Epoch 2/2\n",
      "40/40 [==============================] - 6s 153ms/step - loss: 0.3066 - acc: 0.8958 - val_loss: 0.1858 - val_acc: 0.9527\n"
     ]
    }
   ],
   "source": [
    "fitted_model = loaded_model.fit_generator(training_data,\n",
    "                        steps_per_epoch = 40,\n",
    "                        epochs = 2,\n",
    "                        validation_data = testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/wall1.png'\n",
    "image2 = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/underpass2.jpg'\n",
    "image3 = 'C:/Users/asa_f/Desktop/SJSU/257/Project/Final Data/Final Data/blacknwhite/underpass1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict_classes(\"your_test_data here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
